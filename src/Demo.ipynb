{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For CPVR OpenAccess website\n",
    "e.g: \n",
    "`paper_url`\n",
    "https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper.html\n",
    "\n",
    "`pdf_url`\n",
    "https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://pythonexamples.org/python-regex-check-if-string-starts-with-specific-word/\n",
    "from typing import Tuple\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def process_url_CPVRoa(url: str) -> Tuple[str]:\n",
    "    \"\"\"\n",
    "    Open Access url can be splitted into 5 parts:\n",
    "    start: 'https://openaccess.thecvf.com/'\n",
    "    context: 'content_CVPR_2020/'\n",
    "    pg_type: '/html/'\n",
    "    name: 'Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper'\n",
    "    end: '.html'\n",
    "    ==> url = start + context + pg_type + name + end\n",
    "    \"\"\"\n",
    "    def get_paper_id(url) -> str:\n",
    "        \"\"\"\n",
    "        Can parse either main url (paper_url) or pdf_url to find paper_id\n",
    "        paper_id in the form of: (context + name)\n",
    "        eg: \"content_CVPR_2020/Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper\"\n",
    "        \"\"\"\n",
    "        while \"/\" in url:\n",
    "            slash_idx = url.find(\"/\")\n",
    "            url = url[slash_idx + 1 :]\n",
    "            # stop after slash until \"content_CVPR...\"\n",
    "            flag = re.search('^content', url)\n",
    "            if flag != None:\n",
    "                break\n",
    "        if url.endswith(\".html\"):\n",
    "            paper_id = url.replace(\"/html\", \"\").replace(\".html\",\"\")\n",
    "            return paper_id\n",
    "        else:\n",
    "            paper_id = url.replace(\"/papers\", \"\").replace(\".pdf\",\"\")\n",
    "            return paper_id\n",
    "         \n",
    "    def get_pg_from_paper_id(paper_id: str, parse_mode=\"abs\") -> str:\n",
    "        start = 'https://openaccess.thecvf.com/'\n",
    "        context, name = paper_id.split('/')\n",
    "        if parse_mode == \"abs\":\n",
    "            pg_type = '/html/'\n",
    "            end = '.html'\n",
    "        if parse_mode == \"pdf\":\n",
    "            pg_type = '/papers/'\n",
    "            end = '.pdf'\n",
    "        url = start + context + pg_type + name + end\n",
    "        return url\n",
    "        \n",
    "    paper_id = get_paper_id(url)\n",
    "    if \"/html\" in url:\n",
    "        ## abstract page\n",
    "        paper_url = url\n",
    "        pdf_url = get_pg_from_paper_id(paper_id, parse_mode=\"pdf\")\n",
    "        return paper_id, paper_url, pdf_url\n",
    "    elif \"/papers\" in url:\n",
    "        ## pdf page\n",
    "        paper_url = get_pg_from_paper_id(paper_id, parse_mode=\"abs\")\n",
    "        pdf_url = url\n",
    "        return paper_id, paper_url, pdf_url\n",
    "    else:\n",
    "        logger.error(\"URL not supported\")\n",
    "        raise Exception(\"URL not supported\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG    Starting new HTTPS connection (1): openaccess.thecvf.com:443 (connectionpool.py:937)\n",
      "DEBUG    https://openaccess.thecvf.com:443 \"GET /content_CVPR_2019/html/Li_Finding_Task-Relevant_Features_for_Few-Shot_Learning_by_Category_Traversal_CVPR_2019_paper.html HTTP/1.1\" 200 2137 (connectionpool.py:433)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paper_id': 'content_CVPR_2019/Li_Finding_Task-Relevant_Features_for_Few-Shot_Learning_by_Category_Traversal_CVPR_2019_paper',\n",
       " 'paper_url': 'https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Finding_Task-Relevant_Features_for_Few-Shot_Learning_by_Category_Traversal_CVPR_2019_paper.html',\n",
       " 'pdf_url': 'https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Finding_Task-Relevant_Features_for_Few-Shot_Learning_by_Category_Traversal_CVPR_2019_paper.pdf',\n",
       " 'title': 'Finding Task-Relevant Features for Few-Shot Learning by Category Traversal',\n",
       " 'authors': ['Hongyang Li',\n",
       "  'David Eigen',\n",
       "  'Samuel Dodge',\n",
       "  'Matthew Zeiler',\n",
       "  'Xiaogang Wang'],\n",
       " 'abstract': 'Few-shot learning is an important area of research.  Conceptually, humans are readily able to understand new concepts given just a few examples, while in more pragmatic terms, limited-example training situations are common practice. Recent effective approaches to few-shot learning employ a metric-learning framework to learn a feature similarity comparison between a query (test) example, and the few support (training) examples.  However, these approaches treat each support class independently from one another, never looking at the entire task as a whole.  Because of this, they are constrained to use a single set of features for all possible test-time tasks, which hinders the ability to distinguish the most relevant dimensions for the task at hand.  In this work, we introduce a Category Traversal Module that can be inserted as a plug-and-play module into most metric-learning based few-shot learners.  This component traverses across the entire support set at once, identifying task-relevant features based on both intra-class commonality and inter-class uniqueness in the feature space.  Incorporating our module improves performance considerably (5%-10% relative) over baseline systems on both miniImageNet and tieredImageNet benchmarks, with overall performance competitive with the most recent state-of-the-art systems.',\n",
       " 'bibtex': '@InProceedings{Li_2019_CVPR,\\nauthor = {Li, Hongyang and Eigen, David and Dodge, Samuel and Zeiler, Matthew and Wang, Xiaogang},\\ntitle = {Finding Task-Relevant Features for Few-Shot Learning by Category Traversal},\\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\nmonth = {June},\\nyear = {2019}\\n}\\n'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_paper_CPVRoa(url: str):\n",
    "    response = requests.get(url)\n",
    "    # make soup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    try:\n",
    "        paper_id, paper_url, pdf_url = process_url_CPVRoa(url)\n",
    "    except Exception as err:\n",
    "        logger.error(err)\n",
    "        raise Exception(\"URL not supported\")\n",
    "        \n",
    "    # make paper dict \n",
    "    paper_dict = {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"paper_url\": paper_url,\n",
    "            \"pdf_url\": pdf_url,\n",
    "        }\n",
    "\n",
    "    ##### TITLE\n",
    "    result = soup.find(\"div\", id=\"papertitle\")\n",
    "    tmp = [i.string for i in result]\n",
    "    paper_title = tmp.pop()\n",
    "    paper_dict[\"title\"] = paper_title.strip()\n",
    "    \n",
    "    ##### AUTHORS\n",
    "    result = soup.find(\"div\", id=\"authors\")\n",
    "    main_content = str(result.contents[2])\n",
    "    authors_str = main_content[6:-8]\n",
    "    author_list = [author.lstrip() for author in authors_str.split(',')]\n",
    "    paper_dict[\"authors\"] = author_list\n",
    "    \n",
    "    ##### ABSTRACT\n",
    "    result = soup.find(\"div\", id=\"abstract\")\n",
    "    tmp = [i.string for i in result]\n",
    "    paper_abstract = tmp.pop()\n",
    "    tmp = paper_abstract.split(\"\\n\")\n",
    "    paper_abstract = \" \".join(tmp)\n",
    "    paper_dict[\"abstract\"] = paper_abstract.lstrip()\n",
    "    \n",
    "    ##### Bibtex\n",
    "    result = str(soup.find(\"div\", {\"class\": \"bibref\"}))\n",
    "    bibtex = result[21:-6]\n",
    "    bibtex = bibtex.replace(\"<br/>\", \"\")\n",
    "    paper_dict[\"bibtex\"] = bibtex\n",
    "    return paper_dict\n",
    "\n",
    "get_paper_CPVRoa('https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Finding_Task-Relevant_Features_for_Few-Shot_Learning_by_Category_Traversal_CVPR_2019_paper.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For OpenReview Website\n",
    "e.g:\n",
    "`paper_url` https://openreview.net/forum?id=nIAxjsniDzg\n",
    "\n",
    "`pdf_url` https://openreview.net/pdf?id=nIAxjsniDzg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nIAxjsniDzg',\n",
       " 'https://openreview.net/forum?id=nIAxjsniDzg',\n",
       " 'https://openreview.net/pdf?id=nIAxjsniDzg')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_url_OpenReview(url: str) -> Tuple[str]:\n",
    "    \"\"\"\n",
    "    Open Review url can be splitted into 5 parts:\n",
    "    start: 'https://openreview.net/'\n",
    "    pg_type: 'forum' or 'pdf'\n",
    "    mid: '?id='\n",
    "    paper_id: 'nlAxjsniDzg'\n",
    "    ==> url = start + pg_type + mid + paper_id\n",
    "    \"\"\"\n",
    "    def get_paper_id(url) -> str:\n",
    "        while \"/\" in url:\n",
    "            slash_idx = url.find(\"/\")\n",
    "            url = url[slash_idx + 1 :]\n",
    "        idx = url.find('=')\n",
    "        paper_id = url[idx+1:]\n",
    "        return paper_id\n",
    "         \n",
    "    def get_pg_from_paper_id(paper_id: str, parse_mode=\"abs\") -> str:\n",
    "        start = 'https://openreview.net/'\n",
    "        mid = '?id='\n",
    "        if parse_mode == \"abs\":\n",
    "            pg_type = 'forum'\n",
    "        if parse_mode == \"pdf\":\n",
    "            pg_type = '/papers/'\n",
    "        url = start + pg_type + mid + paper_id\n",
    "        return url\n",
    "        \n",
    "    paper_id = get_paper_id(url)\n",
    "    if \"forum\" in url:\n",
    "        ## abstract page\n",
    "        paper_url = url\n",
    "        pdf_url = get_pg_from_paper_id(paper_id, parse_mode=\"pdf\")\n",
    "        return paper_id, paper_url, pdf_url\n",
    "    elif \"pdf\" in url:\n",
    "        ## pdf page\n",
    "        paper_url = get_pg_from_paper_id(paper_id, parse_mode=\"abs\")\n",
    "        pdf_url = url\n",
    "        return paper_id, paper_url, pdf_url\n",
    "    else:\n",
    "        logger.error(\"URL not supported\")\n",
    "        raise Exception(\"URL not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG    Starting new HTTPS connection (1): openreview.net:443 (connectionpool.py:937)\n",
      "DEBUG    https://openreview.net:443 \"GET /forum?id=nIAxjsniDzg HTTP/1.1\" 200 None (connectionpool.py:433)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'paper_id': 'nIAxjsniDzg',\n",
       " 'paper_url': 'https://openreview.net/forum?id=nIAxjsniDzg',\n",
       " 'pdf_url': 'https://openreview.net//papers/?id=nIAxjsniDzg',\n",
       " 'title': 'What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study',\n",
       " 'keywords': ['Reinforcement learning', 'continuous control'],\n",
       " 'authors': ['Marcin Andrychowicz',\n",
       "  'Anton Raichuk',\n",
       "  'Piotr Stańczyk',\n",
       "  'Manu Orsini',\n",
       "  'Sertan Girgin',\n",
       "  'Raphaël Marinier',\n",
       "  'Leonard Hussenot',\n",
       "  'Matthieu Geist',\n",
       "  'Olivier Pietquin',\n",
       "  'Marcin Michalski',\n",
       "  'Sylvain Gelly',\n",
       "  'Olivier Bachem'],\n",
       " 'abstract': 'In recent years, reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom\\'20]. As a step towards filling that gap, we implement >50 such ``\"choices\" in a unified on-policy deep actor-critic framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250\\'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for the training of on-policy deep actor-critic RL agents.',\n",
       " 'summary': 'We conduct a large-scale empirical study that provides insights and practical recommendations for the training of on-policy deep actor-critic RL agents.',\n",
       " 'bibtex': '@inproceedings{\\nandrychowicz2021what,\\ntitle={What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study},\\nauthor={Marcin Andrychowicz and Anton Raichuk and Piotr Sta{\\\\\\'n}czyk and Manu Orsini and Sertan Girgin and Rapha{\\\\\"e}l Marinier and Leonard Hussenot and Matthieu Geist and Olivier Pietquin and Marcin Michalski and Sylvain Gelly and Olivier Bachem},\\nbooktitle={International Conference on Learning Representations},\\nyear={2021},\\nurl={https://openreview.net/forum?id=nIAxjsniDzg}\\n}'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def get_paper_OpenReview(url: str):\n",
    "    response = requests.get(url)\n",
    "    # make soup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        paper_id, paper_url, pdf_url = process_url_OpenReview(url)\n",
    "    except Exception as err:\n",
    "        logger.error(err)\n",
    "        raise Exception(\"URL not supported\")\n",
    "        \n",
    "    # make paper dict \n",
    "    paper_dict = {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"paper_url\": paper_url,\n",
    "            \"pdf_url\": pdf_url,\n",
    "        }\n",
    "    \n",
    "    # Ref: https://stackoverflow.com/questions/52392246/how-to-convert-class-bs4-element-resultset-to-json-in-python-using-builtin-o\n",
    "    # All data json\n",
    "    result = soup.find(\"script\", id=\"__NEXT_DATA__\")\n",
    "    tmp = [i.string for i in result]\n",
    "    all_data_bs4 = tmp.pop()\n",
    "    # convert to json/dict\n",
    "    all_data_json = json.loads(str(all_data_bs4))\n",
    "    # The \"props\" dict will contain all useful info\n",
    "    main_dict = all_data_json[\"props\"][\"pageProps\"]['forumNote']['content']\n",
    "    \n",
    "    ##### TITLE\n",
    "    paper_dict[\"title\"] = main_dict[\"title\"]\n",
    "    \n",
    "    #### KEYWORDS\n",
    "    paper_dict[\"keywords\"] = main_dict[\"keywords\"]\n",
    "    \n",
    "    ##### AUTHORS\n",
    "    paper_dict[\"authors\"] = main_dict[\"authors\"]\n",
    "    \n",
    "    ##### ABSTRACT\n",
    "    paper_dict[\"abstract\"] = main_dict[\"abstract\"]\n",
    "    \n",
    "    ##### One-sentence_summary\n",
    "    paper_dict[\"summary\"] = main_dict[\"one-sentence_summary\"]\n",
    "    \n",
    "    ##### Bibtex\n",
    "    paper_dict[\"bibtex\"] = main_dict[\"_bibtex\"]\n",
    "    return paper_dict\n",
    "\n",
    "get_paper_OpenReview('https://openreview.net/forum?id=nIAxjsniDzg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
